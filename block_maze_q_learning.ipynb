{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "block_maze_q-learning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwqlIDkqe428"
      },
      "source": [
        "事前準備(共通)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P58IPdw8Ydz_"
      },
      "source": [
        "# 使用するパッケージの宣言\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AImOrxGj1mTS"
      },
      "source": [
        "# dire.csvを読み込む　255*4\n",
        "dire = np.genfromtxt('dire.csv', delimiter=',')\n",
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lqxyO-1gIZl"
      },
      "source": [
        "# print(dire)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zng2NhWn1ng4"
      },
      "source": [
        "theta_0 = np.array(dire)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoFDZkqDgJ1t"
      },
      "source": [
        "# print(theta_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWbYCp9L0JcT"
      },
      "source": [
        "# 方策パラメータthetaを行動方策piに変換する関数の定義\n",
        "def simple_convert_into_pi_from_theta(theta):\n",
        "\n",
        "    [m, n] = theta.shape  # thetaの行列サイズを取得\n",
        "    pi = np.zeros((m, n))\n",
        "    for i in range(0, m):\n",
        "        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # 各マスの四方向の確率を計算\n",
        "\n",
        "    pi = np.nan_to_num(pi)  # nanを0に変換\n",
        "\n",
        "    return pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_hSOeQv0Ng4"
      },
      "source": [
        "# 初期の方策pi_0を求める\n",
        "pi_0 = simple_convert_into_pi_from_theta(theta_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV2JLEzf0P53"
      },
      "source": [
        "# print(pi_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyp0C6kmewDw"
      },
      "source": [
        "事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0FNWE1Yy1zO"
      },
      "source": [
        "# 初期の行動価値関数Qを設定\n",
        "\n",
        "[a, b] = theta_0.shape  # 行と列の数をa, bに格納\n",
        "Q = np.random.rand(a, b) * theta_0 * 0.1\n",
        "# *theta0をすることで要素ごとに掛け算をし、Qの壁方向の値がnanになる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2KBYAsHy7IW"
      },
      "source": [
        "# ε-greedy法を実装\n",
        "\n",
        "def get_action(s, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "\n",
        "    # 行動を決める\n",
        "    if np.random.rand() < epsilon:\n",
        "        # εの確率でランダムに動く\n",
        "        next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
        "    else:\n",
        "        # Qの最大値の行動を採用する\n",
        "        next_direction = direction[np.nanargmax(Q[s, :])]\n",
        "\n",
        "    # 行動をindexに変換\n",
        "    if next_direction == \"up\":\n",
        "        action = 0\n",
        "    elif next_direction == \"right\":\n",
        "        action = 1\n",
        "    elif next_direction == \"down\":\n",
        "        action = 2\n",
        "    elif next_direction == \"left\":\n",
        "        action = 3\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def get_s_next(s, a, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    next_direction = direction[a]  # 行動aの方向\n",
        "\n",
        "    # 行動から次の状態を決める\n",
        "    if next_direction == \"up\":\n",
        "        s_next = s - 17  # 上に移動するときは状態の数字が17小さくなる\n",
        "    elif next_direction == \"right\":\n",
        "        s_next = s + 1  # 右に移動するときは状態の数字が1大きくなる\n",
        "    elif next_direction == \"down\":\n",
        "        s_next = s + 17  # 下に移動するときは状態の数字が17大きくなる\n",
        "    elif next_direction == \"left\":\n",
        "        s_next = s - 1  # 左に移動するときは状態の数字が1小さくなる\n",
        "\n",
        "    return s_next"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMHCMUMiy9a3"
      },
      "source": [
        "# Q学習による行動価値関数Qの更新\n",
        "def Q_learning(s, a, r, s_next, Q, eta, gamma):\n",
        "    if s_next == 246:  # ゴールした場合\n",
        "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
        "    else:\n",
        "        Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next,: ]) - Q[s, a])\n",
        "    return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8miyI68zBFm"
      },
      "source": [
        "# Q学習で迷路を解く関数の定義、状態と行動の履歴および更新したQを出力\n",
        "\n",
        "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n",
        "    s = 238  # スタート地点\n",
        "    a = a_next = get_action(s, Q, epsilon, pi)  # 初期の行動\n",
        "    s_a_history = [[238, np.nan]]  # エージェントのj[状態,行動]を記録するリスト\n",
        "\n",
        "    while (1):  # ゴールするまでループ\n",
        "        a = a_next  # 行動更新\n",
        "\n",
        "        s_a_history[-1][1] = a\n",
        "        # 最新のs_a_history（index=-1）に行動を代入\n",
        "\n",
        "        s_next = get_s_next(s, a, Q, epsilon, pi)\n",
        "        # 次の状態を格納\n",
        "\n",
        "        s_a_history.append([s_next, np.nan])\n",
        "        # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
        "\n",
        "        # 報酬を与え,　次の行動を求める\n",
        "        if s_next == 246:\n",
        "            r = 1  # ゴールにたどり着いたなら報酬を与える\n",
        "            a_next = np.nan\n",
        "        else:\n",
        "            r = 0\n",
        "            a_next = get_action(s_next, Q, epsilon, pi)\n",
        "            # 次の行動a_nextを求める\n",
        "\n",
        "        # 価値関数を更新\n",
        "        Q = Q_learning(s, a, r, s_next, Q, eta, gamma)\n",
        "\n",
        "        # 終了判定\n",
        "        if s_next == 246:  # ゴール地点なら終了\n",
        "            break\n",
        "        else:\n",
        "            s = s_next\n",
        "\n",
        "    return [s_a_history, Q], s_a_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5U2Yq42i8Zi"
      },
      "source": [
        "実行時間"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmmAmSxMzDq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5601e752-afb7-4ecf-c263-8810587601b1"
      },
      "source": [
        "# Q学習で迷路を解く\n",
        "\n",
        "eta = 0.1  # 学習率\n",
        "gamma = 0.9  # 時間割引率\n",
        "epsilon = 0.5  # ε-greedy法の初期値\n",
        "v = np.nanmax(Q, axis=1)  # 状態ごとに価値の最大値を求める\n",
        "is_continue = True\n",
        "episode = 1\n",
        "\n",
        "#処理時間測定\n",
        "from time import time\n",
        "start_time = time()\n",
        "\n",
        "s_a_history_sum = []\n",
        "\n",
        "while is_continue:  # is_continueがFalseになるまで繰り返す\n",
        "    #print(\"エピソード:\" + str(episode))\n",
        "\n",
        "    # ε-greedyの値を少しずつ小さくする\n",
        "    epsilon = epsilon / 2\n",
        "\n",
        "    # Q学習で迷路を解き、移動した履歴と更新したQを求める\n",
        "    [s_a_history,Q],_ = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
        "\n",
        "    # 状態価値の変化\n",
        "    new_v = np.nanmax(Q, axis=1)  # 状態ごとに行動価値の最大値を求める\n",
        "    #print(np.sum(np.abs(new_v - v)))  # 状態価値の変化を出力\n",
        "    v = new_v\n",
        "\n",
        "    # 100エピソード繰り返す\n",
        "    episode = episode + 1\n",
        "    if episode > 1000:\n",
        "        break\n",
        "\n",
        "#処理時間測定\n",
        "proc_time = time() - start_time\n",
        "print(\"実行時間は：\", proc_time,\"秒\")\n",
        "print(\"最短経路長は\" + str(len(s_a_history)) + \"です\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "実行時間は： 4.319839239120483 秒\n",
            "最短経路長は53です\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDAjpfGLdix9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}